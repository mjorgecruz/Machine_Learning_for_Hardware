{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mjorgecruz/Machine_Learning_for_Hardware/blob/main/m2_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7odb95xjc72"
   },
   "source": [
    "# 1.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Notebook Overview\n",
    "\n",
    "## Background\n",
    "Text and image embeddings are just vectors of numbers (say a 512x1 vector) learned by a model, that capture the gist of a sentence or a picture.\n",
    "\n",
    "Instead of raw pixels or words, each point in the vector encodes a bit of meaning—so two text strings that “feel” similar in meaning end up with vectors that sit close together, and the same goes for visually similar images.\n",
    "\n",
    "Contrastive Language-Image Pretraining (CLIP) enables us to create embeddings for text strings as well as images in a shared embedding space. In this shared embedding space the vector for an image, and the vector of the text description of the image will be close to one another. For example, the image of the cat and the text string \"a cat\" will be encoded into 512x1 vectors, and these vectors will be close to one another compared to an unrelated text string like \"an airplane\"\n",
    "\n",
    "## Goals\n",
    "In this notebook we willl learn how to use CLIP to do the following.\n",
    "1. Learn how to obtain embedding for a text string.\n",
    "2. Compare embeddings of different text strings.\n",
    "3. Learn how to obtain embedding for an image.\n",
    "4. Compare embeddings of different images.\n",
    "5. Compare embeddings of images and text strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXYpNm72niOj"
   },
   "source": [
    "# 2.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBARPzVa_ggz"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from transformers import CLIPTokenizer # Tokenizer for the CLIP model, converts text into tokens\n",
    "from transformers import CLIPProcessor # Preprocessor for CLIP model, handles image/text preprocessing\n",
    "from transformers import CLIPModel     # CLIP model for image-text embedding and similarity tasks\n",
    "\n",
    "import torch                           # PyTorch library, used for tensor operations and GPU computations\n",
    "import torch.nn.functional as F        # Contains functional API for neural network operations (e.g., activations, loss functions)\n",
    "import numpy as np                     # Numerical library for array manipulation and computations\n",
    "\n",
    "import matplotlib.pyplot as plt        # Library for plotting images and visualizations\n",
    "import seaborn as sns                  # Library for creating statistical visualizations\n",
    "\n",
    "from PIL import Image                  # Used for loading and processing images\n",
    "import requests                        # Used to fetch images from URLs\n",
    "from io import BytesIO                 # Enables reading binary data as file-like objects in memory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOsykLXkn5tv"
   },
   "source": [
    "# 3.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "d2b449be872c499383709342de9bf6ad",
      "55624997dca3414e9ce3e83dd223c1b4",
      "85f73e72888c4b42bf7ada3dd7c4371e",
      "7efa18627a21479e8a16027d6483938a",
      "9397e1a0abb4490895508d752b5163a2",
      "107a32e9b26447b0a7a36eb981b7942b",
      "8664d5748bf141b68113cb104c32fff2",
      "de5ac47f8c744586962a22bfb03e8a41",
      "04d448c3838b48078ca3330018aacf62",
      "d1b52e298cb84cf0b0ae7568db63e6c6",
      "24ed045bdbff499c8a6f8b20760be31d",
      "0e6aa14b5e9d484289111ebb68deb81d",
      "687477586bb24c92978037e57e9eb8f3",
      "669d36c0907848bf98a5cb866af0f28d",
      "818520e405514ea5b299920eac7248f8",
      "19a3cf0bcaa6439b96576955b919f8a3",
      "21229c69aa124f26a851463767ee49e1",
      "8863122717c341c0b4ddfbc2e66085bc",
      "11153e9288ce44e28d7eda8dd38015d0",
      "32683338c7164960b05c62007d7c32ae",
      "8d28af20b44d4a2fad96c88b33853bed",
      "d579ffd516f945099a972629c9d41f8d"
     ]
    },
    "executionInfo": {
     "elapsed": 5030,
     "status": "ok",
     "timestamp": 1749236116579,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "PacMh_gDoMn5",
    "outputId": "e46d5635-7161-44c9-aeb1-a467a98f7280"
   },
   "outputs": [],
   "source": [
    "# Define the model name for the CLIP variant (Vision Transformer - base, 32x32 patches)\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Load the pre-trained CLIP model from Hugging Face\n",
    "model = CLIPModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky6slVmOoX1h"
   },
   "source": [
    "# 4.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Tokenize Text Strings\n",
    "We first convert the text string into *tokens.* Roughly speaking, a token is a numerical representation of a word. When a string is tokenized it is padded with start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1749236389477,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "rG9EoiNKqfNg",
    "outputId": "de1e1efe-27e0-44bd-8bfa-82ee6564a691"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer associated with the specified CLIP model\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a list of text descriptions to embed\n",
    "text = [\"a donut\", \"a cookie\", \"an airplane\", \"a cat\"]\n",
    "\n",
    "# Tokenize and preprocess the text inputs with padding to ensure equal sequence lengths\n",
    "inputs = tokenizer(text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Unpack the inputs\n",
    "input_ids = inputs.input_ids  # Tokenized and encoded text input IDs\n",
    "\n",
    "# Print information about the unpacked inputs\n",
    "print(\"Input IDs (shape):\", input_ids.shape)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtxuCHOtrsAv"
   },
   "source": [
    "# 5.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Find Text Embeddings\n",
    "The CLIP model takes in tokenized strings and returns an 512 length embedding for every string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1749236579933,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "s9mFm96J_pRX",
    "outputId": "86b79e49-29ac-48c8-dd55-44d8f94b4617"
   },
   "outputs": [],
   "source": [
    "# Compute text embeddings without tracking gradients (inference mode)\n",
    "with torch.no_grad():\n",
    "    # Obtain text embeddings (feature vectors) from the CLIP model\n",
    "    text_embeddings = model.get_text_features(**inputs)\n",
    "\n",
    "# Print the shape of the resulting text embeddings tensor\n",
    "# The shape is [number_of_texts, embedding_dimension]\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKdDfPDSpcwQ"
   },
   "source": [
    "# 6.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Calculate Cosine Similarity\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Why did the two vectors start dating after math class?*\n",
    "\n",
    "*Because when they calculated their cosine similarity, they realized they were practically aligned—it was love at first dot-product!*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Cosine similarity** measures how similar (aligned) two vectors are by looking only at the angle between them, not their length.\n",
    "\n",
    "* Imagine each vector as an arrow from the origin.\n",
    "* The cosine of the angle \\$\\theta\\$ between the arrows is\n",
    "\n",
    "$$\n",
    "\\operatorname{cosine\\_sim}(\\mathbf{a}, \\mathbf{b}) \\;=\\; \\cos(\\theta) \\;=\\;\n",
    "\\frac{\\mathbf{a}\\cdot\\mathbf{b}}\n",
    "     {\\lVert\\mathbf{a}\\rVert\\,\\lVert\\mathbf{b}\\rVert}\n",
    "$$\n",
    "\n",
    "* A value of **+1** means the arrows point in exactly the same direction (perfect similarity), **0** means they’re orthogonal (no similarity), and **–1** means they point in opposite directions (complete dissimilarity).\n",
    "\n",
    "Because it ignores magnitude, cosine similarity is ideal for comparing text or image embeddings where direction captures meaning and length may just scale with word count or pixel intensity.\n",
    "\n",
    "The calculated cosine similarity between strings are displayed using a color coded matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1749236813448,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "RJm0NKpCCGhg",
    "outputId": "1e1544e2-a874-48a2-8b19-dab48ef5a135"
   },
   "outputs": [],
   "source": [
    "# Text_embeddings is a tensor of shape [n, d], where:\n",
    "# - n = number of text prompts\n",
    "# - d = embedding dimension\n",
    "\n",
    "# Compute the n x n cosine similarity matrix between all pairs of embeddings\n",
    "# text_embeddings[:, None, :] reshapes embeddings to [n, 1, d]\n",
    "# text_embeddings[None, :, :] reshapes embeddings to [1, n, d]\n",
    "# cosine_similarity calculates similarity along the last dimension (d)\n",
    "cosine_similarity = F.cosine_similarity(\n",
    "    text_embeddings[:, None, :],    # Shape: [n, 1, d]\n",
    "    text_embeddings[None, :, :],    # Shape: [1, n, d]\n",
    "    dim=2                           # Calculate similarity along embedding dimension d\n",
    ").cpu().numpy()                     # Move to CPU and convert tensor to NumPy array for plotting\n",
    "\n",
    "# Initialize a matplotlib figure with specified size (width=6, height=4)\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create a heatmap visualization using seaborn to display the cosine similarity matrix\n",
    "sns.heatmap(\n",
    "    cosine_similarity,              # Matrix to visualize (n x n similarity scores)\n",
    "    annot=True,                     # Annotate each cell with numeric similarity value\n",
    "    fmt=\".2f\",                      # Format annotations to two decimal places\n",
    "    cmap=\"coolwarm\",                # Colormap for heatmap indicating negative/positive similarities\n",
    "    xticklabels=text,               # Label x-axis with the original text prompts\n",
    "    yticklabels=text                # Label y-axis with the original text prompts\n",
    ")\n",
    "\n",
    "# Set the plot title with font size 14\n",
    "plt.title(\"Cosine Similarity Matrix\", fontsize=14)\n",
    "\n",
    "# Label x-axis as \"Text Embeddings\"\n",
    "plt.xlabel(\"Text Embeddings\")\n",
    "\n",
    "# Label y-axis as \"Text Embeddings\"\n",
    "plt.ylabel(\"Text Embeddings\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5dS-NW_uirx"
   },
   "source": [
    "# 7.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQ_PC-dKRk83"
   },
   "outputs": [],
   "source": [
    "# Utility function for displaying images with labels\n",
    "def plot_images(images, labels):\n",
    "  n = len(images)                        # Number of images loaded successfully\n",
    "  fig, axes = plt.subplots(1, n)         # Create subplots with one row and n columns\n",
    "  # Loop through each subplot axis, image, and its label to display them\n",
    "  for ax, img, lbl in zip(axes, images, labels):\n",
    "      ax.imshow(img)                     # Display the image on the axis\n",
    "      ax.set_title(lbl)                  # Set the title of the subplot to the image label\n",
    "      ax.axis(\"off\")                     # Turn off axis ticks and labels for clarity\n",
    "\n",
    "  plt.tight_layout()                     # Adjust layout to prevent overlap\n",
    "  plt.show()                             # Show the image plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga5NuDgQusF9"
   },
   "source": [
    "# 8.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Load and Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPhQn8_7rC3j"
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained CLIP processor for handling images and text preprocessing\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269,
     "referenced_widgets": [
      "d7d3ae81ea47406b9e8db443db75b52f",
      "707f59b6543e4767b4844d9a2290b649",
      "c087513682a545b3a58afc2177d0bf30",
      "e0cd97f9bcd5498da8bb5cd627dda03f",
      "6bf333bfcde143818c2f87913b1e0b4a",
      "f711b37a02984d3c851533f6d48aea28",
      "50fe54f44a554a51a4662647216180e9",
      "bb4572fa21b54e63afad93107272d921",
      "73d4fe658e1848279a23ff4cd3210b67",
      "768d646038634877b15257937abb0fdb",
      "5005f27dbca84cc89d585080f5499a56"
     ]
    },
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1749237014474,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "SM0HPnRr_Nbn",
    "outputId": "a0b040dd-a97f-49b5-9389-5550c6066d51"
   },
   "outputs": [],
   "source": [
    "# Dictionary containing labels and their corresponding image URLs\n",
    "image_urls = {\n",
    "    \"a donut\": \"https://learnopencv.com/wp-content/uploads/2025/03/donut.jpeg\",\n",
    "    \"a cookie\": \"https://learnopencv.com/wp-content/uploads/2025/03/cookie.jpeg\",\n",
    "    \"an airplane\": \"https://learnopencv.com/wp-content/uploads/2025/03/airplane.jpeg\",\n",
    "    \"a cat\": \"https://learnopencv.com/wp-content/uploads/2025/03/cat.jpeg\"\n",
    "}\n",
    "\n",
    "# Extract the list of labels from the dictionary keys\n",
    "labels = list(image_urls.keys())\n",
    "\n",
    "# Define a robust function to load images from URLs\n",
    "def load_image(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}        # Set headers to avoid blocking by web servers\n",
    "    response = requests.get(url, headers=headers)  # Request the image from the URL\n",
    "    response.raise_for_status()                    # Raise an error if the download fails\n",
    "    # Open the downloaded image, convert it to RGB format, and return the PIL Image\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# Initialize empty lists for successfully loaded images and their labels\n",
    "images = []\n",
    "\n",
    "# Loop through each label to load the associated image\n",
    "for label in labels:\n",
    "    try:\n",
    "        img = load_image(image_urls[label])  # Load image from URL\n",
    "        images.append(img)                   # Append loaded image to images list\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # If an image fails to load, print an error message\n",
    "        print(f\"Failed to load {label}: {e}\")\n",
    "\n",
    "# Display the loaded images in a single row using matplotlib\n",
    "plot_images(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1viu93xu0Pb"
   },
   "source": [
    "# 9.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Calculate Image Emdeddings & Display Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1749237200822,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "a60tkLUvVskZ",
    "outputId": "8a427769-7cc2-4a1d-bab0-b4aad24777c1"
   },
   "outputs": [],
   "source": [
    "# Preprocess images using CLIP processor to prepare for embedding generation\n",
    "image_inputs = processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Generate image embeddings using the CLIP model\n",
    "  image_embeddings = model.get_image_features(**image_inputs)\n",
    "\n",
    "# Print the shape of the resulting image embeddings tensor\n",
    "# The shape is [number_of_images, embedding_dimension]\n",
    "print(image_embeddings.shape)\n",
    "\n",
    "# Compute similarity matrix\n",
    "img_similarity = F.cosine_similarity(image_embeddings[:, None, :], image_embeddings[None, :, :], dim=2).cpu().numpy()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(img_similarity, annot=True, xticklabels=labels, yticklabels=labels, cmap=\"coolwarm\")\n",
    "plt.xlabel(\"Image Embeddings\")\n",
    "plt.ylabel(\"Image Embeddings\")\n",
    "plt.title(\"CLIP Image-Image Similarity Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQn7NZFyu8HS"
   },
   "source": [
    "# 10.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Calculate Image-Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1749237333955,
     "user": {
      "displayName": "Satya Mallick",
      "userId": "01070973255527645696"
     },
     "user_tz": 420
    },
    "id": "ONdoGBnGFD1U",
    "outputId": "e75e0613-c07b-459d-ff99-b629cf33457e"
   },
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "txt_image_similarity = F.cosine_similarity(text_embeddings[:, None, :], image_embeddings[None, :, :], dim=2).cpu().numpy()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(txt_image_similarity, annot=True, xticklabels=labels, yticklabels=labels, cmap=\"coolwarm\")\n",
    "plt.xlabel(\"Text Embeddings\")\n",
    "plt.ylabel(\"Image Embeddings\")\n",
    "plt.title(\"CLIP Image-Text Similarity Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seKHCCPWvCl9"
   },
   "source": [
    "# 11.󠀠󠀮󠁽󠁝󠁝󠁝󠁝 Conclusion\n",
    "Before you leave this notebook make sure you understand the following concepts.\n",
    "1. Tokenization\n",
    "2. Text embedding\n",
    "3. Image embedding\n",
    "4. Cosine similarity\n",
    "\n",
    "That's all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PvSYvGzvUKs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1_1ye3GqGWRx0RYz2zYdkVCU4imheKBA4",
     "timestamp": 1747512812499
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
